{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPYyxzBR2Crl"
   },
   "source": [
    "# Introduction\n",
    "## **WELCOME!!üëãüëã**\n",
    "\n",
    "This notebook is the first exercise in the Learning Path for the Maven Course: [Build Multi-Agent Applications - A Bootcamp](https://maven.com/aggregate-intellect/llm-systems). Let‚Äôs get started on developing our first LLM powered application together!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKcG4ead2QkK"
   },
   "source": [
    "## Objective üéØ:\n",
    "\n",
    "The goal is to create a LLM Application that:\n",
    "- Answer question about best practices of RAG as defined in AISC Substack Article: [How to do retrieval augmented generation (RAG) right!](https://aisc.substack.com/p/how-to-do-retrieval-augmented-generation)\n",
    "- Develop a front-end UI using Chainlit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUQzuKqi3FiN"
   },
   "source": [
    "# Libraries Required for the Excercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9bN-bxW_YKD_"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "! pip install langsmith langchain-community langchain chromadb tiktoken langchain_openai chainlit\n",
    "!npm install localtunnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11gfAIUZ3OyS"
   },
   "source": [
    "## Setup Required - APIs\n",
    "\n",
    "In this notebook you'll be using OpenAI models for reasoning and generation, langsmith for tracing and langchain for orchestration, chainlit for UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pMZ8o2O0f5m3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langsmith import utils\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "import openai\n",
    "from langsmith import traceable\n",
    "\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_x6ixYzDhHDU"
   },
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# define your langchain project name here\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"maven-course-learning-path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXUkTMOoOIdV",
    "outputId": "63ce4bfd-c3f8-48f8-c0dd-1962f6d06e51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your OPENAI_API_KEY:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
      "Enter your LANGCHAIN_API_KEY:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "def _set_env(key: str):\n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = getpass.getpass(f\"Enter your {key}:\")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o8MLaztcgys9",
    "outputId": "ac0993cf-bf2d-4b64-e539-7416c8407fa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Tracing is enabled: True\n"
     ]
    }
   ],
   "source": [
    "print('Is Tracing is enabled:', utils.tracing_is_enabled())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj6LK5BT3noD"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HvOj4GD1YaMH"
   },
   "outputs": [],
   "source": [
    "### INDEX\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "# Load\n",
    "url = \"https://aisc.substack.com/p/how-to-do-retrieval-augmented-generation\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Index\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqsYoSkM3zNZ"
   },
   "source": [
    "## RAG Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iTZmdjZTZoEv"
   },
   "outputs": [],
   "source": [
    "# RAG\n",
    "class RagApp:\n",
    "\n",
    "    def __init__(self, retriever, model: str = \"gpt-4o-mini\", prompt: str = \"rlm/rag-prompt\"):\n",
    "        self._retriever = retriever\n",
    "        self._llm = ChatOpenAI(model = model)\n",
    "        # Prompt Link: https://smith.langchain.com/hub/rlm/rag-prompt)\n",
    "        self._prompt = hub.pull(prompt)\n",
    "\n",
    "    @traceable()\n",
    "    def retrieve_docs(self, question):\n",
    "        return self._retriever.invoke(question)\n",
    "\n",
    "    @traceable()\n",
    "    def get_answer(self, question: str):\n",
    "        rag_chain = (\n",
    "            {\"context\": self._retriever, \"question\": RunnablePassthrough()}\n",
    "            | self._prompt\n",
    "            | self._llm\n",
    "        )\n",
    "\n",
    "        response = rag_chain.invoke(question)\n",
    "        similar = self.retrieve_docs(question)\n",
    "        #Evaluators will expect \"answer\" and \"contexts\"\n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"contexts\": [str(doc) for doc in similar],\n",
    "        }\n",
    "\n",
    "rag_bot = RagApp(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oSo2hGbla8Q0"
   },
   "outputs": [],
   "source": [
    "rag_response = rag_bot.get_answer(\"Name all individuals whose presentation insipred this blog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bGfG00Njnu1R",
    "outputId": "d589d0f8-80fd-4657-f8ba-01668c04a6ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'The individuals whose presentations inspired the blog are Pai, Ian Yu, Nikhil Varghese, Percy Chen, and Suhas.',\n",
       " 'contexts': ['page_content=\\'Pai, Ian Yu, Nikhil Varghese, and Percy Chen. Some of the thought processes presented here are borrowed with permission from the content of Suhas‚Äôs book, Designing Large Language Models Applications. The early drafts of this article and the accompanying visuals were provided by Mohsin Iqbal.Thanks\\' metadata={\\'content_type\\': \\'text/html; charset=utf-8\\', \\'description\\': \\'My client: ‚Äúhey, we have this vector db + LLM RAG thing and it‚Äôs not working‚Äù. And my answer is often ‚Äúpull in the chair and sit down, we need to talk about how robust software is built\".\\', \\'language\\': \\'en\\', \\'source\\': \\'https://aisc.substack.com/p/how-to-do-retrieval-augmented-generation\\', \\'title\\': \\'How to do retrieval augmented generation (RAG) right!\\'}',\n",
       "  'page_content=\\'Jul 10Thanks for the article and your speech at AI Practioner event! Expand full commentReplyShare¬© 2024 Aggregate Intellect Inc.Privacy ‚àô Terms ‚àô Collection notice Start WritingGet the appSubstack is the home for great cultureShareCopy linkFacebookEmailNotesMore\\' metadata={\\'content_type\\': \\'text/html; charset=utf-8\\', \\'description\\': \\'My client: ‚Äúhey, we have this vector db + LLM RAG thing and it‚Äôs not working‚Äù. And my answer is often ‚Äúpull in the chair and sit down, we need to talk about how robust software is built\".\\', \\'language\\': \\'en\\', \\'source\\': \\'https://aisc.substack.com/p/how-to-do-retrieval-augmented-generation/comments\\', \\'title\\': \\'Comments - How to do retrieval augmented generation (RAG) right!\\'}',\n",
       "  'page_content=\\'vendor blog posts online, does not work. Instead the architecture used has to be tightly designed around the nuances of the workflow it‚Äôs expected to augment. Humans don\\'t simply regurgitate memorized facts. We gather information, analyze it, and then use language to communicate our understanding.\\' metadata={\\'content_type\\': \\'text/html; charset=utf-8\\', \\'description\\': \\'My client: ‚Äúhey, we have this vector db + LLM RAG thing and it‚Äôs not working‚Äù. And my answer is often ‚Äúpull in the chair and sit down, we need to talk about how robust software is built\".\\', \\'language\\': \\'en\\', \\'source\\': \\'https://aisc.substack.com/p/how-to-do-retrieval-augmented-generation\\', \\'title\\': \\'How to do retrieval augmented generation (RAG) right!\\'}',\n",
       "  'page_content=\\'with AI Jul 10Thanks for the article and your speech at AI Practioner event! Expand full commentReplyShareTopLatestDiscussionsNo postsReady for more?Subscribe¬© 2024 Aggregate Intellect Inc.Privacy ‚àô Terms ‚àô Collection notice Start WritingGet the appSubstack is the home for great cultureShareCopy\\' metadata={\\'content_type\\': \\'text/html; charset=utf-8\\', \\'description\\': \\'My client: ‚Äúhey, we have this vector db + LLM RAG thing and it‚Äôs not working‚Äù. And my answer is often ‚Äúpull in the chair and sit down, we need to talk about how robust software is built\".\\', \\'language\\': \\'en\\', \\'source\\': \\'https://aisc.substack.com/p/how-to-do-retrieval-augmented-generation\\', \\'title\\': \\'How to do retrieval augmented generation (RAG) right!\\'}']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nX3Z5Vp4C45"
   },
   "source": [
    "## Create Front-end with Chainlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztrNeOc36fc2",
    "outputId": "f4600139-8a70-440d-9cd7-c8ff94ad29de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.runnable import Runnable\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import chainlit as cl\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "import os\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def get_retriever():\n",
    "    # Load\n",
    "    url = \"https://aisc.substack.com/p/how-to-do-retrieval-augmented-generation\"\n",
    "    loader = RecursiveUrlLoader(\n",
    "        url=url,\n",
    "        max_depth=10,\n",
    "        extractor=lambda x: BeautifulSoup(x, \"html.parser\").text\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Split\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Embed\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "    # Return retriever with search kwargs\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def start():\n",
    "    # Initialize the chain components\n",
    "    retriever = get_retriever()\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", streaming=True)\n",
    "    prompt = hub.pull('rlm/rag-prompt')\n",
    "    # Create the chain\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": lambda x: format_docs(retriever.get_relevant_documents(x[\"question\"])),\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "    )\n",
    "\n",
    "    # Store the chain in the user session\n",
    "    cl.user_session.set(\"chain\", rag_chain)\n",
    "\n",
    "    # Send an initial message\n",
    "    await cl.Message(content=\"Hi! I'm ready to help you with questions about AISC substack article on RAG. What would you like to know?\").send()\n",
    "\n",
    "@cl.on_message\n",
    "async def main(message: cl.Message):\n",
    "    chain = cl.user_session.get(\"chain\")  # Get the chain from the session\n",
    "\n",
    "    if chain is None:\n",
    "        await cl.Message(content=\"Error: Chain not initialized. Please restart the chat.\").send()\n",
    "        return\n",
    "\n",
    "    msg = cl.Message(content=\"\")\n",
    "\n",
    "    async with cl.Step(\"Fetched Content ü§î\"):\n",
    "        async for chunk in chain.astream(\n",
    "            {\"question\": message.content},\n",
    "            config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n",
    "        ):\n",
    "            await msg.stream_token(chunk.content)\n",
    "\n",
    "    await msg.send()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cl.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcYWZOt743SU"
   },
   "source": [
    "### Access Chainlit service running on the colab instance using [localtunnel](https://github.com/localtunnel/localtunnel)\n",
    "\n",
    "#### You'll need to enter the following password to access the dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLKysCei9XOe",
    "outputId": "16773fa3-1c59-4ec0-8b11-511c53190100"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your password is: \n",
      "34.46.111.8"
     ]
    }
   ],
   "source": [
    "print('your password is: ')\n",
    "!curl ipecho.net/plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XE5XngNi6b5P",
    "outputId": "09c8b799-8ecb-4f05-a64b-8cb2bb9006d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-23 07:31:15 - Created default config file at /content/.chainlit/config.toml\n",
      "2024-11-23 07:31:15 - Created default translation directory at /content/.chainlit/translations\n",
      "2024-11-23 07:31:15 - Created default translation file at /content/.chainlit/translations/ta.json\n",
      "2024-11-23 07:31:15 - Created default translation file at /content/.chainlit/translations/he-IL.json\n",
      "2024-11-23 07:31:15 - Created default translation file at /content/.chainlit/translations/gu.json\n",
      "2024-11-23 07:31:15 - Created default translation file at /content/.chainlit/translations/kn.json\n",
      "2024-11-23 07:31:15 - Created default translation file at /content/.chainlit/translations/hi.json\n",
      "2024-11-23 07:31:15 - Created default translation file at /content/.chainlit/translations/mr.json\n",
      "2024-11-23 07:31:15 - Created default translation file at /content/.chainlit/translations/ml.json\n",
      "2024-11-23 07:31:15 - Created default translation file at /content/.chainlit/translations/en-US.json\n",
      "2024-11-23 07:31:15 - Created default translation file at /content/.chainlit/translations/te.json\n",
      "2024-11-23 07:31:15 - Created default translation file at /content/.chainlit/translations/bn.json\n",
      "2024-11-23 07:31:15 - Created default translation file at /content/.chainlit/translations/zh-CN.json\n",
      "your url is: https://slow-webs-create.loca.lt\n",
      "2024-11-23 07:31:19 - Created default chainlit markdown file at /content/chainlit.md\n",
      "2024-11-23 07:31:19 - Your app is available at http://localhost:8000\n",
      "2024-11-23 07:31:38 - Translated markdown file for en-US not found. Defaulting to chainlit.md.\n",
      "2024-11-23 07:31:44 - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-11-23 07:31:46 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "/content/app.py:47: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  \"context\": lambda x: format_docs(retriever.get_relevant_documents(x[\"question\"])),\n",
      "2024-11-23 07:31:56 - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-11-23 07:31:57 - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!chainlit run app.py & npx localtunnel --port 8000"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
